from torch.utils import data
from torchvision import transforms
import os
import os.path
from PIL import Image
import random
import numpy as np
from utils import cv_utils
import pickle
import cv2

def get_loader(config, dataset_name='EM', mode='train', num_workers=1):
    """Build and return a data loader."""

    dataset = AusDataset(config, mode, dataset_name)
    if mode=='train':
        bt=config.batch_size
    elif mode=='test':
        bt=1
    else:
        bt = 15


    data_loader = data.DataLoader(dataset=dataset,
                                  batch_size=bt,
                                  # shuffle=False,
                                  shuffle=(mode == 'train'),
                                  drop_last=True,
                                  num_workers=num_workers)
    # else:
    #     raise ValueError("Dataset [%s] not recognized." % dataset_name)


    return data_loader


class DatasetBase(data.Dataset):
    def __init__(self):
        super(DatasetBase, self).__init__()
        self._name = 'BaseDataset'
        self.root = None

        # self.create_transform()

        self.IMG_EXTENSIONS = [
            '.jpg', '.JPG', '.jpeg', '.JPEG',
            '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
        ]

    @property
    def name(self):
        return self._name

    @property
    def path(self):
        return self.root



    def is_image_file(self, filename):
        return any(filename.endswith(extension) for extension in self.IMG_EXTENSIONS)

    def is_csv_file(self, filename):
        return filename.endswith('.csv')

    def get_all_files_in_subfolders(self, dir, is_file):
        images = []
        assert os.path.isdir(dir), '%s is not a valid directory' % dir

        for root, _, fnames in sorted(os.walk(dir)):
            for fname in fnames:
                if is_file(fname):
                    path = os.path.join(root, fname)
                    images.append(path)
        return images


class AusDataset(DatasetBase):
    def __init__(self, config, mode, dataset_name):
        super(AusDataset, self).__init__()
        self.dataset_name = dataset_name
        self.config=config

        self.imgs_dir =config.aus_root

        self.attr_path=os.path.join(config.aus_root,config.aus_file)
        self.mode = mode
        self.root=self.config.aus_root


        self.create_transform()
        # read dataset
        self.read_dataset()

    def __getitem__(self, index):
        assert (index < self.dataset_size)

        # start_time = time.time()
        real_img = None
        real_cond = None
        while real_img is None or real_cond is None:
            # if sample randomly: overwrite index
            if not self.config.serial_batches:
                index = random.randint(0, self.dataset_size - 1)

            # get sample data
            sample_id = self.ids[index]

            real_img, real_img_path = self.get_img_by_id(sample_id)     ##have modification here
            real_cond = self.get_cond_by_id(sample_id)                  ##have modification here

            if real_img is None:
                print('error reading image %s, skipping sample' % sample_id)
            if real_cond is None:
                print('error reading aus %s, skipping sample' % sample_id)

        # index = random.randint(0, self.dataset_size - 1)
        # sample_id = self.ids[index]
        # ref_img1, _ = self.get_img_by_id(sample_id)
        # ref_img1= self.transform(Image.fromarray(ref_img1))
        #
        # index = random.randint(0, self.dataset_size - 1)
        # sample_id = self.ids[index]
        # ref_img2, _ = self.get_img_by_id(sample_id)
        # ref_img2 = self.transform(Image.fromarray(ref_img2))

        desired_cond = self.generate_random_cond()                      ##have modification here

        # transform data
        img = self.transform(Image.fromarray(real_img))

        # pack data
        sample = {'real_img': img,
                  'real_cond': real_cond,
                  'desired_cond': desired_cond,
                  'sample_id': sample_id,
                  'real_img_path': real_img_path
                  }

        # print (time.time() - start_time)

        return sample

    def __len__(self):
        return self.dataset_size

    def read_dataset(self):
        # read ids
        ids_filename = self.config.aus_train_ids_file if self.mode=='train' else self.config.aus_test_ids_file
        ids_filepath = os.path.join(self.root, ids_filename)
        self.ids = self.read_ids(ids_filepath)

        # read aus
        conds_filepath = self.attr_path
        self.conds = self.read_conds(conds_filepath)          ## conditions are generated by openface.

        self.ids = list(set(self.ids).intersection(set(self.conds.keys())))  ## comment this time temporarily

        self.dataset_size = len(self.ids)


        print('Finished preprocessing the {} dataset...'.format(self.name))

    def create_transform(self):
        if self.mode=='train':
            transform_list = [transforms.RandomHorizontalFlip(),
                              transforms.ToTensor(),
                              transforms.Normalize(mean=[0.5, 0.5, 0.5],
                                                   std=[0.5, 0.5, 0.5]),
                              ]
        else:
            transform_list = [transforms.ToTensor(),
                              transforms.Normalize(mean=[0.5, 0.5, 0.5],
                                                   std=[0.5, 0.5, 0.5]),
                              ]
        self.transform = transforms.Compose(transform_list)

    def read_ids(self, file_path):
        with open(file_path, 'rb') as f:
            ids = pickle.load(f, encoding='utf-8')
            f.close()
        # ids=pickle.load(file_path)
        # ids = np.loadtxt(file_path, delimiter='\t', dtype=np.str,)
        return ids

    def read_conds(self, file_path):
        with open(file_path, 'rb') as f:
            return pickle.load(f, encoding='latin1')

    def get_cond_by_id(self, id):
        if id in self.conds:
            new_conds=self.conds[id][:17]/5.0
            return new_conds
        else:
            return None

    def get_img_by_id(self, id):
        img = None
        for suffix in self.IMG_EXTENSIONS:
            filepath =self.imgs_dir+ id+suffix
            if os.path.exists(filepath):
                img=cv_utils.read_cv2_img(filepath)
                break
        return img, filepath

    def generate_random_cond(self):
        cond = None
        while cond is None:
            rand_sample_id = self.ids[random.randint(0, self.dataset_size - 1)]
            cond = self.get_cond_by_id(rand_sample_id)
            cond += np.random.uniform(-0.1, 0.1, 17)
        return cond




